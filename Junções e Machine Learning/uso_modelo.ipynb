{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Levi\\AppData\\Local\\Temp\\ipykernel_125412\\3468391338.py:18: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  df1['timestamp'] = df1['datetime'].view('int64') / 1e9  # Converte para segundos (float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Acurácia: 0.6453\n",
      "Sensibilidade (Recall): 0.8754\n",
      "AUC: 0.6402\n",
      "Especificidade: 0.4050\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4956 - loss: 1.9748\n",
      "Avaliação do Modelo: [2.0147488117218018, 0.48929527401924133]\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      1.00      0.66      3291\n",
      "         1.0       0.00      0.00      0.00      3435\n",
      "\n",
      "    accuracy                           0.49      6726\n",
      "   macro avg       0.24      0.50      0.33      6726\n",
      "weighted avg       0.24      0.49      0.32      6726\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[1333 1958]\n",
      " [ 428 3007]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Carregar o dataset\n",
    "df1 = pd.read_csv('Subsampling.csv')\n",
    "\n",
    "# Convertendo a coluna 'Data' para datetime\n",
    "df1['Data'] = pd.to_datetime(df1['Data'], format='%Y-%m-%d')\n",
    "\n",
    "# Criando uma coluna datetime completa combinando data e hora\n",
    "df1['datetime'] = pd.to_datetime(df1['Data'].astype(str) + ' ' + df1['Hora (UTC)'].astype(str) + ':00', format='%Y-%m-%d %H:%M')\n",
    "\n",
    "# Transformar datetime em uma coluna numérica (timestamp float)\n",
    "df1['timestamp'] = df1['datetime'].view('int64') / 1e9  # Converte para segundos (float)\n",
    "\n",
    "# Removendo colunas desnecessárias para o modelo\n",
    "df1.drop(columns=['Data', 'Hora (UTC)', 'datetime'], inplace=True)\n",
    "\n",
    "# Certificando que todos os tipos de dados sejam float64 para normalização adequada\n",
    "df1 = df1.astype(np.float64)\n",
    "\n",
    "# Separar as features (X) e o alvo (y)\n",
    "X = df1.drop(columns=['num_ocorrencias'])\n",
    "y = df1['num_ocorrencias'].values\n",
    "\n",
    "# Carregar o modelo salvo\n",
    "modelo_caminho = 'Modelo_A64-S87-AUC64-E40.keras'\n",
    "modelo = tf.keras.models.load_model(modelo_caminho)\n",
    "\n",
    "# Normalizar as features\n",
    "scaler_X = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "# Normalizar o target\n",
    "scaler_y = MinMaxScaler()\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Reformatar X para [amostras, timesteps, características] - ajustando para LSTM\n",
    "X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Dividir os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Reverter a transformação dos valores para a escala original\n",
    "try:\n",
    "    y_pred = scaler_y.inverse_transform(y_pred)\n",
    "    y_test = scaler_y.inverse_transform(y_test.reshape(-1, 1))\n",
    "except NameError:\n",
    "    print(\"Scalers não definidos; usando previsões e testes sem transformação inversa.\")\n",
    "\n",
    "# Definir um limiar para converter previsões contínuas em classes binárias\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred > threshold).astype(float)\n",
    "y_test_binary = (y_test > threshold).astype(float)\n",
    "\n",
    "# Avaliação do modelo\n",
    "accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "recall = recall_score(y_test_binary, y_pred_binary)\n",
    "roc_auc = roc_auc_score(y_test_binary, y_pred_binary)\n",
    "conf_matrix = confusion_matrix(y_test_binary, y_pred_binary)\n",
    "\n",
    "# Cálculo da especificidade\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Exibir resultados de avaliação\n",
    "print(f'Acurácia: {accuracy:.4f}')\n",
    "print(f'Sensibilidade (Recall): {recall:.4f}')\n",
    "print(f'AUC: {roc_auc:.4f}')\n",
    "print(f'Especificidade: {specificity:.4f}')\n",
    "\n",
    "# Avaliar o modelo com os dados de teste\n",
    "avaliacao = modelo.evaluate(X_test, y_test)\n",
    "print(f\"Avaliação do Modelo: {avaliacao}\")\n",
    "\n",
    "# Fazer previsões com o modelo\n",
    "previsoes = modelo.predict(X_test)\n",
    "\n",
    "# Para classificação binária, comparar com o threshold\n",
    "if previsoes.shape[1] == 1:\n",
    "    previsoes_classes = (previsoes > threshold).astype(int)\n",
    "else:\n",
    "    previsoes_classes = previsoes.argmax(axis=1)  # Para classificação com mais de duas classes\n",
    "\n",
    "# Gerar relatório de classificação\n",
    "print(\"Relatório de Classificação:\")\n",
    "print(classification_report(y_test_binary, previsoes_classes))\n",
    "\n",
    "# Matriz de Confusão\n",
    "print(\"Matriz de Confusão:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Caminho do modelo salvo\n",
    "modelo_caminho = 'Modelo_A64-S87-AUC64-E40.keras'\n",
    "\n",
    "# Carregar o modelo treinado\n",
    "modelo = tf.keras.models.load_model(modelo_caminho)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARAÇAO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Carregar os novos dados (substitua pelo caminho correto dos dados)\n",
    "novos_dados = pd.read_csv('novos_dados.csv')\n",
    "\n",
    "# Convertendo as colunas de data e hora (ajuste conforme a estrutura dos novos dados)\n",
    "novos_dados['Data'] = pd.to_datetime(novos_dados['Data'], format='%Y-%m-%d')\n",
    "novos_dados['datetime'] = pd.to_datetime(novos_dados['Data'].astype(str) + ' ' + novos_dados['Hora (UTC)'].astype(str) + ':00')\n",
    "novos_dados['timestamp'] = novos_dados['datetime'].astype('int64') / 1e9\n",
    "\n",
    "# Remover colunas que não são necessárias para a previsão\n",
    "novos_dados.drop(columns=['Data', 'Hora (UTC)', 'datetime'], inplace=True)\n",
    "\n",
    "# Garantir que todas as colunas sejam do tipo float64\n",
    "novos_dados = novos_dados.astype(np.float64)\n",
    "\n",
    "# Selecionar as features (X) para predição\n",
    "X_novos = novos_dados.drop(columns=['num_ocorrencias'], errors='ignore')  # Remova 'num_ocorrencias' se presente\n",
    "\n",
    "# Normalizar os novos dados com o mesmo scaler usado no treinamento\n",
    "X_novos_scaled = scaler_X.transform(X_novos)  # Certifique-se que 'scaler_X' está carregado do treino\n",
    "\n",
    "# Ajustar a forma do X para o formato LSTM [samples, timesteps, features]\n",
    "X_novos_scaled = X_novos_scaled.reshape((X_novos_scaled.shape[0], 1, X_novos_scaled.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDIÇAO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer previsões com os novos dados processados\n",
    "previsoes = modelo.predict(X_novos_scaled)\n",
    "\n",
    "# Desnormalizar as previsões se necessário\n",
    "previsoes_desnormalizadas = scaler_y.inverse_transform(previsoes)\n",
    "\n",
    "# Aplicar o limiar para converter previsões contínuas em binárias\n",
    "threshold = 0.5 \n",
    "previsoes_binarias = (previsoes_desnormalizadas > threshold).astype(int)\n",
    "\n",
    "# Exibir resultados\n",
    "print(\"Previsões Binárias (0 = Sem Queda, 1 = Com Queda):\")\n",
    "print(previsoes_binarias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API  com Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Inicializa o Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Carrega o modelo e scalers\n",
    "modelo = tf.keras.models.load_model('Modelo_A64-S87-AUC64-E40.keras')\n",
    "scaler_X = MinMaxScaler()  # Assumindo que esses scalers foram salvos e carregados corretamente\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "@app.route('/prever', methods=['POST'])\n",
    "def prever():\n",
    "    dados = request.json\n",
    "    df = pd.DataFrame(dados)\n",
    "    # (Pre-processamento como nos passos anteriores)\n",
    "    # Fazer a predição e retornar o resultado\n",
    "    previsoes = modelo.predict(df)  # Ajuste conforme necessário\n",
    "    return jsonify(previsoes.tolist())\n",
    "\n",
    "# Executa o app\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
